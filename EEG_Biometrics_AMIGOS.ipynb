	
cells	
0	
cell_type	"markdown"
metadata	
id	"title_cell"
source	
0	"# EEG-Based Biometric Authentication Using AMIGOS Dataset\n"
1	"\n"
2	"## Abstract\n"
3	"This notebook presents a comprehensive study on EEG-based biometric authentication using the AMIGOS dataset. \n"
4	"The research investigates the effectiveness of various machine learning classifiers for person identification \n"
5	"based on EEG signal features extracted during emotional stimuli presentation.\n"
6	"\n"
7	"## Authors\n"
8	"- [Author Name]\n"
9	"- [Institution]\n"
10	"- [Email]\n"
11	"\n"
12	"## Publication Information\n"
13	"- **Dataset**: AMIGOS - A dataset for Affect, Personality and Mood research on Individuals and Groups\n"
14	"- **Preprocessing**: Bandpass filtering, artifact removal, feature extraction\n"
15	"- **Classification**: Multiple ML algorithms with cross-validation\n"
16	"- **Evaluation**: Accuracy, F1-score, FAR (False Acceptance Rate), FRR (False Rejection Rate)\n"
17	"\n"
18	"## Reproducibility Statement\n"
19	"This notebook is designed for full reproducibility. All random seeds are set, dependencies are documented,\n"
20	"and the code is structured for clarity and reusability."
1	
cell_type	"markdown"
metadata	
id	"setup_section"
source	
0	"# 1. Environment Setup and Dependencies\n"
1	"\n"
2	"This section sets up the computational environment and installs required dependencies."
2	
cell_type	"code"
execution_count	null
metadata	
id	"system_info"
outputs	[]
source	
0	"# System Information and Hardware Specifications\n"
1	"import psutil\n"
2	"import platform\n"
3	"from datetime import datetime\n"
4	"\n"
5	"def display_system_information():\n"
6	'    """\n'
7	"    Display comprehensive system information for reproducibility.\n"
8	"    \n"
9	"    Returns:\n"
10	"        dict: System specifications including CPU, memory, and platform details\n"
11	'    """\n'
12	'    cpu_frequency = psutil.cpu_freq().current if psutil.cpu_freq() else "Unknown"\n'
13	"    physical_cores = psutil.cpu_count(logical=False)\n"
14	"    logical_cores = psutil.cpu_count(logical=True)\n"
15	"    total_ram_gb = psutil.virtual_memory().total / (1024**3)\n"
16	"    \n"
17	"    system_specs = {\n"
18	"        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n"
19	"        'platform': platform.platform(),\n"
20	"        'python_version': platform.python_version(),\n"
21	"        'cpu_frequency_mhz': cpu_frequency,\n"
22	"        'physical_cores': physical_cores,\n"
23	"        'logical_cores': logical_cores,\n"
24	"        'total_ram_gb': round(total_ram_gb, 2)\n"
25	"    }\n"
26	"    \n"
27	'    print("=== SYSTEM SPECIFICATIONS ===")\n'
28	"    for key, value in system_specs.items():\n"
29	`        print(f"{key.replace('_', ' ').title()}: {value}")\n`
30	'    print("=" * 30)\n'
31	"    \n"
32	"    return system_specs\n"
33	"\n"
34	"# Display system information\n"
35	"system_info = display_system_information()"
3	
cell_type	"code"
execution_count	null
metadata	
id	"dependency_installation"
outputs	[]
source	
0	"# Install required packages with specific versions for reproducibility\n"
1	"import subprocess\n"
2	"import sys\n"
3	"\n"
4	"def install_package(package_name, version=None):\n"
5	'    """\n'
6	"    Install a Python package with optional version specification.\n"
7	"    \n"
8	"    Args:\n"
9	"        package_name (str): Name of the package to install\n"
10	"        version (str, optional): Specific version to install\n"
11	'    """\n'
12	"    if version:\n"
13	'        package_spec = f"{package_name}=={version}"\n'
14	"    else:\n"
15	"        package_spec = package_name\n"
16	"    \n"
17	"    try:\n"
18	'        subprocess.check_call([sys.executable, "-m", "pip", "install", package_spec])\n'
19	'        print(f"✓ Successfully installed {package_spec}")\n'
20	"    except subprocess.CalledProcessError as e:\n"
21	'        print(f"✗ Failed to install {package_spec}: {e}")\n'
22	"\n"
23	"# Required packages for EEG biometric analysis\n"
24	"required_packages = [\n"
25	"    ('numpy', '1.24.3'),\n"
26	"    ('pandas', '2.0.3'),\n"
27	"    ('scipy', '1.11.1'),\n"
28	"    ('scikit-learn', '1.3.0'),\n"
29	"    ('matplotlib', '3.7.2'),\n"
30	"    ('seaborn', '0.12.2'),\n"
31	"    ('antropy', '0.1.9'),  # Entropy-based time series analysis\n"
32	"    ('xgboost', '1.7.6'),\n"
33	"    ('tqdm', '4.65.0'),    # Progress bars\n"
34	"    ('torch', '2.0.1'),    # PyTorch for deep learning components\n"
35	"]\n"
36	"\n"
37	'print("Installing required packages...")\n'
38	"for package, version in required_packages:\n"
39	"    install_package(package, version)\n"
40	"\n"
41	'print("\\n=== DEPENDENCY INSTALLATION COMPLETE ===")'
4	
cell_type	"markdown"
metadata	
id	"imports_section"
source	
0	"# 2. Library Imports and Configuration\n"
1	"\n"
2	"Comprehensive import of all required libraries with clear organization."
5	
cell_type	"code"
execution_count	null
metadata	
id	"library_imports"
outputs	[]
source	
0	"# Standard Library Imports\n"
1	"import os\n"
2	"import pickle\n"
3	"import random\n"
4	"import time\n"
5	"import warnings\n"
6	"from itertools import islice\n"
7	"from typing import Dict, List, Tuple, Any, Optional\n"
8	"\n"
9	"# Scientific Computing Libraries\n"
10	"import numpy as np\n"
11	"import pandas as pd\n"
12	"from scipy import signal\n"
13	"from scipy.signal import welch, resample\n"
14	"from scipy.stats import skew, kurtosis, mannwhitneyu\n"
15	"\n"
16	"# Machine Learning Libraries\n"
17	"from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n"
18	"from sklearn.linear_model import LogisticRegression\n"
19	"from sklearn.svm import SVC\n"
20	"from sklearn.neighbors import KNeighborsClassifier\n"
21	"from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n"
22	"from sklearn.preprocessing import StandardScaler, LabelEncoder\n"
23	"from sklearn.feature_selection import SelectKBest, f_classif\n"
24	"from sklearn.metrics import (\n"
25	"    accuracy_score, precision_score, recall_score, f1_score,\n"
26	"    confusion_matrix, classification_report\n"
27	")\n"
28	"from sklearn.base import clone\n"
29	"\n"
30	"# Specialized Libraries\n"
31	"import antropy as entropy_analysis  # Entropy-based time series analysis\n"
32	"from xgboost import XGBClassifier\n"
33	"\n"
34	"# Deep Learning Libraries\n"
35	"import torch\n"
36	"import torch.nn as nn\n"
37	"from torch.utils.data import DataLoader\n"
38	"from torch.utils.tensorboard import SummaryWriter\n"
39	"\n"
40	"# Visualization Libraries\n"
41	"import matplotlib.pyplot as plt\n"
42	"import seaborn as sns\n"
43	"\n"
44	"# Progress Tracking\n"
45	"from tqdm import tqdm\n"
46	"\n"
47	"# Configuration Settings\n"
48	"warnings.filterwarnings('ignore', category=FutureWarning)\n"
49	"plt.style.use('seaborn-v0_8')\n"
50	'sns.set_palette("husl")\n'
51	"\n"
52	"# Set random seeds for reproducibility\n"
53	"RANDOM_SEED = 42\n"
54	"np.random.seed(RANDOM_SEED)\n"
55	"random.seed(RANDOM_SEED)\n"
56	"torch.manual_seed(RANDOM_SEED)\n"
57	"if torch.cuda.is_available():\n"
58	"    torch.cuda.manual_seed(RANDOM_SEED)\n"
59	"    torch.cuda.manual_seed_all(RANDOM_SEED)\n"
60	"\n"
61	"# Device configuration\n"
62	"DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
63	'print(f"Using device: {DEVICE}")\n'
64	"\n"
65	"# Global configuration constants\n"
66	"EEG_SAMPLING_RATE = 128  # Hz - AMIGOS dataset sampling rate\n"
67	"N_JOBS = -1  # Use all available cores for parallel processing\n"
68	"\n"
69	'print("✓ All libraries imported successfully")\n'
70	'print(f"✓ Random seed set to: {RANDOM_SEED}")\n'
71	'print(f"✓ EEG sampling rate: {EEG_SAMPLING_RATE} Hz")'
metadata	
kernelspec	
display_name	"Python 3"
language	"python"
name	"python3"
language_info	
codemirror_mode	
name	"ipython"
version	3
file_extension	".py"
mimetype	"text/x-python"
name	"python"
nbconvert_exporter	"python"
pygments_lexer	"ipython3"
version	"3.8.10"
accelerator	"GPU"
gpuType	"T4"
nbformat	4
nbformat_minor	0
